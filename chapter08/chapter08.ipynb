{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP80/100.ipynb のコピー",
      "provenance": [],
      "collapsed_sections": [
        "PnR6u561QW8I",
        "rpR29zZErLhv",
        "hv4gAVHc9Zkj"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_feZb3i1I5qz"
      },
      "source": [
        "# 8章：ニューラルネット\n",
        "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbLnQEMeK-eC"
      },
      "source": [
        "## 70. 単語ベクトルの和による特徴量\n",
        "\n",
        "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例$x_i$の特徴ベクトル$\\boldsymbol{x}_i$を並べた行列$X$と正解ラベルを並べた行列（ベクトル）$Y$を作成したい．\n",
        "\n",
        "$$\n",
        "X = \\begin{pmatrix} \n",
        "  \\boldsymbol{x}_1 \\\\ \n",
        "  \\boldsymbol{x}_2 \\\\ \n",
        "  \\dots \\\\ \n",
        "  \\boldsymbol{x}_n \\\\ \n",
        "\\end{pmatrix} \\in \\mathbb{R}^{n \\times d},\n",
        "Y = \\begin{pmatrix} \n",
        "  y_1 \\\\ \n",
        "  y_2 \\\\ \n",
        "  \\dots \\\\ \n",
        "  y_n \\\\ \n",
        "\\end{pmatrix} \\in \\mathbb{N}^{n}\n",
        "$$\n",
        "\n",
        "\n",
        " ここで，$n$は学習データの事例数であり，$\\boldsymbol x_i \\in \\mathbb{R}^d$と$y_i \\in \\mathbb N$はそれぞれ，$i \\in \\{1, \\dots, n\\}$番目の事例の特徴量ベクトルと正解ラベルを表す．\n",
        " なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．$\\mathbb N_{<4}$で$4$未満の自然数（$0$を含む）を表すことにすれば，任意の事例の正解ラベル$y_i$は$y_i \\in \\mathbb N_{<4}$で表現できる．\n",
        " 以降では，ラベルの種類数を$L$で表す（今回の分類タスクでは$L=4$である）．\n",
        "\n",
        " $i$番目の事例の特徴ベクトル$\\boldsymbol x_i$は，次式で求める．\n",
        "\n",
        " $$\\boldsymbol x_i = \\frac{1}{T_i} \\sum_{t=1}^{T_i} \\mathrm{emb}(w_{i,t})$$\n",
        "\n",
        " ここで，$i$番目の事例は$T_i$個の（記事見出しの）単語列$(w_{i,1}, w_{i,2}, \\dots, w_{i,T_i})$から構成され，$\\mathrm{emb}(w) \\in \\mathbb{R}^d$は単語$w$に対応する単語ベクトル（次元数は$d$）である．  \n",
        " すなわち，**$i$番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものが$\\boldsymbol x_i$である．**今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．$300$次元の単語ベクトルを用いたので，$d=300$である．  \n",
        " $i$番目の事例のラベル$y_i$は，次のように定義する．\n",
        "\n",
        "$$\n",
        "y_i = \\begin{cases}\n",
        "0 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「ビジネス」カテゴリの場合}) \\\\\n",
        "1 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「科学技術」カテゴリの場合}) \\\\\n",
        "2 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「エンターテイメント」カテゴリの場合}) \\\\\n",
        "3 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「健康」カテゴリの場合}) \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n",
        "\n",
        "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
        "\n",
        " + 学習データの特徴量行列: $X_{\\rm train} \\in \\mathbb{R}^{N_t \\times d}$\n",
        " + 学習データのラベルベクトル: $Y_{\\rm train} \\in \\mathbb{N}^{N_t}$\n",
        " + 検証データの特徴量行列: $X_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d}$\n",
        " + 検証データのラベルベクトル: $Y_{\\rm valid} \\in \\mathbb{N}^{N_v}$\n",
        " + 評価データの特徴量行列: $X_{\\rm test} \\in \\mathbb{R}^{N_e \\times d}$\n",
        " + 評価データのラベルベクトル: $Y_{\\rm test} \\in \\mathbb{N}^{N_e}$\n",
        "\n",
        "なお，$N_t, N_v, N_e$はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe6HkQ7NjK7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fc6057-fdac-4e1a-cb38-a2951dc43b87"
      },
      "source": [
        "# データのダウンロード\n",
        "# 記事の見出しとそのカテゴリが入ったデータ\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip NewsAggregatorDataset.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-29 07:11:36--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29224203 (28M) [application/x-httpd-php]\n",
            "Saving to: ‘NewsAggregatorDataset.zip’\n",
            "\n",
            "NewsAggregatorDatas 100%[===================>]  27.87M  57.6MB/s    in 0.5s    \n",
            "\n",
            "2021-06-29 07:11:37 (57.6 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203/29224203]\n",
            "\n",
            "Archive:  NewsAggregatorDataset.zip\n",
            "  inflating: 2pageSessions.csv       \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._2pageSessions.csv  \n",
            "  inflating: newsCorpora.csv         \n",
            "  inflating: __MACOSX/._newsCorpora.csv  \n",
            "  inflating: readme.txt              \n",
            "  inflating: __MACOSX/._readme.txt   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQll8f3Njq04"
      },
      "source": [
        "# データの読み込み、タイトルとカテゴリの抽出、学習データ・検証データ・評価データへの適切な分割\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データの読込\n",
        "df = pd.read_csv('./newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "\n",
        "# タイトルとカテゴリの抽出\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "# データの分割\n",
        "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_CZP81HkkmY"
      },
      "source": [
        "import gdown  # Download a large file from Google Drive.\n",
        "# If you use curl/wget, it fails with a large file because of the security warning from Google Drive.\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# 学習済み単語ベクトルのダウンロード\n",
        "url = \"https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
        "output = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "# ダウンロードファイルのロード\n",
        "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHwxHoGIlaVd"
      },
      "source": [
        "import string\n",
        "import torch\n",
        "\n",
        "def transform_w2v(text):\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))  # str.translate() に使える変換テーブルを返します。\n",
        "  words = text.translate(table).split()  # 記号をスペースに置換後、スペースで分割してリスト化\n",
        "  vec = [model[word] for word in words if word in model]  # 単語が単語ベクトルにあるものなら、テキスト中の単語を1語ずつベクトル化\n",
        "  # 各単語の単語ベクトルを全て足し合わせ、その平均を特徴ベクトルとする\n",
        "  return torch.tensor(sum(vec) / len(vec))  # 平均ベクトルをTensor型に変換して出力"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F9nxX2Mleau"
      },
      "source": [
        "# 記事タイトルを１つづつ取り出し、特徴ベクトルの作成\n",
        "X_train = torch.stack([transform_w2v(text) for text in train['TITLE']])  # Concatenates a sequence of tensors along a new dimension.\n",
        "                                                                         # All tensors need to be of the same size.\n",
        "X_valid = torch.stack([transform_w2v(text) for text in valid['TITLE']])\n",
        "X_test = torch.stack([transform_w2v(text) for text in test['TITLE']])\n",
        "\n",
        "print(X_train.size())  # (記事タイトル数, 単語ベクトルの次元数)\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfTqcie6lioN"
      },
      "source": [
        "# ラベルベクトルの作成...記事カテゴリを正解ラベルに変換\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "y_train = torch.tensor(train['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
        "y_valid = torch.tensor(valid['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
        "y_test = torch.tensor(test['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
        "\n",
        "print(y_train.size())\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GCShFBPtZB0"
      },
      "source": [
        "## 71. 単層ニューラルネットワークによる予測\n",
        "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n",
        "\n",
        "$$ \n",
        "\\hat{y}_1=softmax(x_1W),\\\\\\hat{Y}=softmax(X_{[1:4]}W)\n",
        "$$\n",
        "\n",
        "\n",
        "ただし，$softmax$はソフトマックス関数，$X_{[1:4]}∈\\mathbb{R}^{4×d}$は特徴ベクトル$x_1$,$x_2$,$x_3$,$x_4$を縦に並べた行列である．\n",
        "\n",
        "$$\n",
        "X_{[1:4]}=\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "行列$W \\in \\mathbb{R}^{d \\times L}$は単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．  \n",
        "なお，$\\hat{\\boldsymbol y_1} \\in \\mathbb{R}^L$は未学習の行列$W$で事例$x_1$を分類したときに，各カテゴリに属する確率を表すベクトルである．\n",
        "同様に，$\\hat{Y} \\in \\mathbb{R}^{n \\times L}$は，学習データの事例$x_1, x_2, x_3, x_4$について，各カテゴリに属する確率を行列として表現している．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_tRHP6gsLAy"
      },
      "source": [
        "$d$：単語ベクトルの次元数  \n",
        "$L$：記事のカテゴリ数  \n",
        "$n$：記事数  \n",
        "　 $Y　= 　　X 　W$  \n",
        "$(n×L)　(n×d)(d×L)$  \n",
        "\n",
        "記事のベクトルに重みを掛けて、各カテゴリに属する確率(的な値)を求める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkVy8TNeQ1bq"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):  #  define our neural network by subclassing nn.Module\n",
        "  def __init__(self, input_size, output_size):  # initialize the neural network layers in __init__\n",
        "    super().__init__()\n",
        "    self.liner1 = nn.Linear(input_size, output_size, bias=False)  # The linear layer is a module that applies a linear transformation\n",
        "                                                                # using its stored weights and biases  # バイアス無くても成立はず\n",
        "    nn.init.normal_(self.liner1.weight, 0.0, 1.0)  # 正規乱数で重みを初期化  # 無くても成立はするはず\n",
        "\n",
        "    self.one_liner_stack = nn.Sequential(  # nn.Sequential is an ordered container of modules\n",
        "        self.liner1                        # The input is passed through all the modules in the same order as defined\n",
        "    )\n",
        "\n",
        "  def forward(self, x):  # Every nn.Module subclass implements the forward method\n",
        "    x = self.one_liner_stack(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9kLEEhVRPnI"
      },
      "source": [
        "model = NeuralNetwork(300, 4)  # 単層ニューラルネットワークの初期化\n",
        "\n",
        "# To use the model, we pass it the input data. This executes the model’s forward.\n",
        "# Do not call model.forward() directly!\n",
        "y_hat_1 = torch.softmax(model(X_train[:1]), dim=-1)\n",
        "print(y_hat_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRc8h_MCSDq2"
      },
      "source": [
        "Y_hat = torch.softmax(model(X_train[:4]), dim=-1)\n",
        "print(Y_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2AZ3dWHJqo9"
      },
      "source": [
        "## 72. 損失と勾配の計算\n",
        "\n",
        "学習データの事例x1\n",
        "と事例集合x1,x2,x3,x4\n",
        "に対して，クロスエントロピー損失と，行列W\n",
        "に対する勾配を計算せよ．なお，ある事例xi\n",
        "に対して損失は次式で計算される．\n",
        "\n",
        "$$\n",
        "l_i=−log[事例x_iがy_iに分類される確率]\n",
        "$$\n",
        "\n",
        "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HSJiiwNgAXT"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()  # This criterion combines LogSoftmax and NLLLoss in one single class."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NsZYzXrg4OV"
      },
      "source": [
        "l_1 = criterion(model(X_train[:1]), y_train[:1])  # X_trainは、記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したもの\n",
        "                                                  # model(X_train)はsoftmaxに通す前の(=確率にする前の)出力(カテゴリの数だけある)\n",
        "                                                    # The input is expected to contain raw, unnormalized scores for each class.\n",
        "                                                  # y_trainは、正解カテゴリ名を数字ラベルで表したもの(正解のカテゴリ番号１つでよい)\n",
        "                                                    # ↑one_hot encodingが必要ないってこと？\n",
        "model.zero_grad()  # 勾配をゼロで初期化  # ２回目以降、前の勾配の値が残ったままになるのを防ぐ\n",
        "l_1.backward()  # 勾配を計算\n",
        "print(f'損失: {l_1:.4f}')\n",
        "print(f'勾配:\\n{model.liner1.weight.grad}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kph0bRsdkUJ9"
      },
      "source": [
        "l = loss_fn(model(X_train[:4]), y_train[:4])\n",
        "model.zero_grad()\n",
        "l.backward()\n",
        "print(f'損失: {l:.4f}')\n",
        "print(f'勾配:\\n{model.liner1.weight.grad}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uudzTu5mWLp1"
      },
      "source": [
        "## 73. 確率的勾配降下法による学習\n",
        "\n",
        "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列W\n",
        "を学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n-4HKntv3Ok"
      },
      "source": [
        "from torch.utils.data import Dataset  # Dataset stores the samples and their corresponding labels\n",
        "\n",
        "# A custom Dataset class must implement three functions: __init__, __len__, and __getitem__\n",
        "class NewsDataset(Dataset):  \n",
        "  def __init__(self, X, y):  # datasetの構成要素を指定(データの初期設定)\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):  # The __len__ function returns the number of samples in our dataset\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):  # dataset[idx]で返す値を指定  # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
        "    return [self.X[idx], self.y[idx]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2eqrSqyln8g"
      },
      "source": [
        "# The Dataset retrieves our dataset’s features and labels one sample at a time\n",
        "# While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting.\n",
        "# DataLoader is an iterable that abstracts this complexity for us in an easy API\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Datasetの作成\n",
        "dataset_train = NewsDataset(X_train, y_train)  # 記事数×記事の特徴ベクトル, 記事カテゴリのラベル数字\n",
        "dataset_valid = NewsDataset(X_valid, y_valid)\n",
        "dataset_test = NewsDataset(X_test, y_test)\n",
        "\n",
        "# Dataloaderの作成\n",
        "# DataloaderはDatasetを入力とし、指定したサイズ(batch_size)にまとめたデータを順に取り出すことができます\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)   # データを１つずつ取り出す\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)  # データを1336(検証データの数)ずつ取り出す(?)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)\n",
        "\n",
        "# Dataloaderはfor文で順に取り出すか、またはnext(iter(Dataloader))で次のかたまりを呼び出すことが可能です。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmqjZlKmyqCc"
      },
      "source": [
        "学習の流れ(1エポック)  \n",
        "* モデルの出力を計算(～softmaxの前)\n",
        "* 損失を求める(～CrossEntopyLoss)\n",
        "* 各パラメータについて損失に対する偏微分の値を求める(～backward)\n",
        "* 勾配降下法に基づいてパラメータを最適化する(～SGD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmzzSEFo2n-c"
      },
      "source": [
        "# モデルの定義\n",
        "model = NeuralNetwork(300, 4)\n",
        "\n",
        "# ハイパーパラメータの定義\n",
        "learning_rate = 1e-1\n",
        "# batch_size = 64  # dataloaderがバッチを分けたデータを返してくれる\n",
        "epochs = 10\n",
        "\n",
        "# 損失関数の定義\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyaRjyIb2qE7"
      },
      "source": [
        "def train_loop(dataloader_train, dataloader_valid, model, loss_fn, optimizer):\n",
        "    Loss_train = 0.0\n",
        "    for batch, (X, y) in enumerate(dataloader_train):  # batch : 何バッチ目か。全部で10684バッチ\n",
        "        # 予測と損失の計算\n",
        "        pred_train = model(X)  # size(X) = (1, 300)(記事数, 記事の特徴ベクトル)\n",
        "        loss_train = loss_fn(pred_train, y)\n",
        "        \n",
        "        # バックプロパゲーション\n",
        "        optimizer.zero_grad()  # Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration\n",
        "        loss_train.backward()\n",
        "        optimizer.step()  #  adjust the parameters by the gradients collected in the backward pass\n",
        "\n",
        "        # 損失を記録\n",
        "        Loss_train += loss_train.item()\n",
        "    # バッチ単位の平均損失計算\n",
        "    Loss_train = Loss_train / batch\n",
        "\n",
        "        # if batch % 1000 == 0:  # バッチのセットが1000回取り出されたら、出力を確認\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    # 1エポック毎に検証データの損失計算\n",
        "    # model.eval() \n",
        "    with torch.no_grad():  # 検証中は伝播記録を残さない\n",
        "      X, y = next(iter(dataloader_valid)) # size(X) = (300, 300)(記事数, 記事の特徴ベクトル)\n",
        "      pred_valid = model(X)\n",
        "      loss_valid = loss_fn(pred_valid, y)\n",
        "\n",
        "    # １エポック毎にログを出力\n",
        "    print(f'loss_train: {Loss_train:.4f}, loss_valid: {loss_valid:.4f}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPM01Ys_-ISo"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(dataloader_train, dataloader_valid, model, loss_fn, optimizer)    \n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6uSmaljF8H0"
      },
      "source": [
        "## 74. 正解率の計測\n",
        "\n",
        "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MII59dC4LPtJ"
      },
      "source": [
        "学習した重みで予測をする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y59FWcPsJUMQ"
      },
      "source": [
        "def calculate_accuracy(model, dataloader):\n",
        "    size = len(dataloader.dataset)  # 10684かな？\n",
        "    total, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():  # テスト中は伝播記録をしない\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    return correct / size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXrzpN6BIblM"
      },
      "source": [
        "acc_train = calculate_accuracy(model, dataloader_train)\n",
        "acc_test = calculate_accuracy(model, dataloader_test)\n",
        "print(f'正解率（学習データ）：{(100*acc_train):>0.1f}%')  # >は右詰めの記号\n",
        "print(f'正解率（評価データ）：{(100*acc_test):>0.1f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKnCbpspuexA"
      },
      "source": [
        "## 75. 損失と正解率のプロット\n",
        "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpPgAeLvMoHQ"
      },
      "source": [
        "def calculate_loss_and_accuracy(model, dataloader, loss_fn):\n",
        "    total_loss = len(dataloader)  # バッチ数(損失はバッチ毎(1イテレータ)に1つ求まるので)\n",
        "    total_data = dataloader.batch_size * len(dataloader)  # ※全データ =  (バッチ内のデータ数) * (バッチ数)\n",
        "                                  # 予測結果はバッチ毎に、バッチ内のデータ数だけ求まる\n",
        "    correct, loss = 0, 0\n",
        "\n",
        "    with torch.no_grad():  # テスト中は伝播記録をしない\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            loss += loss_fn(pred, y).item()  # バッチ内に複数データがあるなら、各データの損失の平均を返す?\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    return loss / total_loss, correct / total_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeVN9LbGUZFt"
      },
      "source": [
        "print(dataloader_train.batch_size) # 1バッチ内のデータ数\n",
        "print(len(dataloader_train))  # バッチが何個あるのか\n",
        "print(dataloader_valid.batch_size) \n",
        "print(len(dataloader_valid))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFSZqublNnWY"
      },
      "source": [
        "# モデルの定義\n",
        "model = NeuralNetwork(300, 4)\n",
        "\n",
        "# ハイパーパラメータの定義\n",
        "learning_rate = 1e-1\n",
        "# batch_size = 64  # dataloaderがバッチを分けたデータを返してくれる\n",
        "epochs = 10\n",
        "\n",
        "log_train = []\n",
        "log_valid = []\n",
        "\n",
        "# 損失関数の定義\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy9hcQjcN20z"
      },
      "source": [
        "def train_loop(dataloader_train, dataloader_valid, model, loss_fn, optimizer):\n",
        "    for batch, (X, y) in enumerate(dataloader_train):  # batch : 何バッチ目か。全部で10684バッチ\n",
        "        # 予測と損失の計算\n",
        "        pred_train = model(X)  # size(X) = (1, 300)(記事数, 記事の特徴ベクトル)\n",
        "        loss_train = loss_fn(pred_train, y)\n",
        "        \n",
        "        # バックプロパゲーション\n",
        "        optimizer.zero_grad()  # Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration\n",
        "        loss_train.backward()\n",
        "        optimizer.step()  #  adjust the parameters by the gradients collected in the backward pass\n",
        "\n",
        "    # 各エポックのパラメータ更新が完了するたびに(全データで？)損失と正解率の算出\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataloader_train, loss_fn)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataloader_valid, loss_fn)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    # １エポック毎にログを出力\n",
        "    print(f'loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5XbsETSQ3Z2"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(dataloader_train, dataloader_valid, model, loss_fn, optimizer)    \n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x95WukYfKMN"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# 視覚化\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax[0].plot(np.array(log_train).T[0], label='train')\n",
        "ax[0].plot(np.array(log_valid).T[0], label='valid')\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].legend()\n",
        "ax[1].plot(np.array(log_train).T[1], label='train')\n",
        "ax[1].plot(np.array(log_valid).T[1], label='valid')\n",
        "ax[1].set_xlabel('epoch')\n",
        "ax[1].set_ylabel('accuracy')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnR6u561QW8I"
      },
      "source": [
        "## 76. チェックポイント\n",
        "\n",
        "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoBth1egfjXq"
      },
      "source": [
        "学習途中のパラメータはmodel.state_dict()、最適化アルゴリズムの内部状態はoptimizer.state_dict()でアクセス可能なので、各エポックでエポック数と合わせて保存する処理を追加"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9ZgF8sYfk_U"
      },
      "source": [
        "def train_loop(dataloader_train, dataloader_valid, model, loss_fn, optimizer, epoch):\n",
        "    for batch, (X, y) in enumerate(dataloader_train):  # batch : 何バッチ目か。全部で10684バッチ\n",
        "        # 予測と損失の計算\n",
        "        pred_train = model(X)  # size(X) = (1, 300)(記事数, 記事の特徴ベクトル)\n",
        "        loss_train = loss_fn(pred_train, y)\n",
        "        \n",
        "        # バックプロパゲーション\n",
        "        optimizer.zero_grad()  # Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration\n",
        "        loss_train.backward()\n",
        "        optimizer.step()  #  adjust the parameters by the gradients collected in the backward pass\n",
        "\n",
        "    # 各エポックのパラメータ更新が完了するたびに(全データで？)損失と正解率の算出\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataloader_train, loss_fn)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataloader_valid, loss_fn)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "     # チェックポイントの保存\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    # １エポック毎にログを出力\n",
        "    print(f'loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpR29zZErLhv"
      },
      "source": [
        "## 77. ミニバッチ化\n",
        "\n",
        "問題76のコードを改変し，B\n",
        "事例ごとに損失・勾配を計算し，行列W\n",
        "の値を更新せよ（ミニバッチ化）．B\n",
        "の値を1,2,4,8,…\n",
        "と変化させながら，1エポックの学習に要する時間を比較せよ．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS_bqgkPgWOY"
      },
      "source": [
        "バッチサイズを変えるごとにすべての処理を書くのは大変なので、Dataloaderの作成以降の処理をtrain_modelとして関数化し、バッチサイズを含むいくつかのパラメータを引数として設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohln70OmgW_L"
      },
      "source": [
        "import time\n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, loss_fn, optimizer, epochs):\n",
        "  # dataloaderの作成\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "  # 学習\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(epochs):\n",
        "    # 開始時刻の記録\n",
        "    s_time = time.time()\n",
        "\n",
        "    # 訓練モード\n",
        "    # model.train()\n",
        "    for X, y in dataloader_train:\n",
        "      # 勾配をゼロで初期化\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # 損失と正解率の算出\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataloader_train, loss_fn)  # dataとlossの位置が参考と逆なので注意\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataloader_valid, loss_fn)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    # チェックポイントの保存\n",
        "    # torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    # 終了時刻の記録\n",
        "    e_time = time.time()\n",
        "\n",
        "    # ログを出力\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s65Eaa4iXM0"
      },
      "source": [
        "# datasetの作成\n",
        "dataset_train = NewsDataset(X_train, y_train)\n",
        "dataset_valid = NewsDataset(X_valid, y_valid)\n",
        "\n",
        "# モデルの定義\n",
        "model = NeuralNetwork(300, 4)\n",
        "\n",
        "# 損失関数の定義\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "# モデルの学習\n",
        "for batch_size in [2 ** i for i in range(11)]:\n",
        "  print(f'バッチサイズ: {batch_size}')\n",
        "  log = train_model(dataset_train, dataset_valid, batch_size, model, loss_fn, optimizer, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d0hDwZCj4UV"
      },
      "source": [
        "Pythonはデータをまとめて処理する方が計算時間が速くなる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv4gAVHc9Zkj"
      },
      "source": [
        "## 78. GPU上での学習\n",
        "\n",
        "問題77のコードを改変し，GPU上で学習を実行せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsVAIqdKkH-W"
      },
      "source": [
        "GPUを指定する引数deviceをcalculate_loss_and_accuracy、train_modelに追加します。\n",
        "それぞれの関数内で、モデルおよび入力TensorをGPUに送る処理を追加し、deviceにcudaを指定すればGPUを使用することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwKPeloSkIrf"
      },
      "source": [
        "def calculate_loss_and_accuracy(model, dataloader, loss_fn, device):\n",
        "    total_loss = len(dataloader)  # バッチ数(損失はバッチ毎(1イテレータ)に1つ求まるので)\n",
        "    total_data = dataloader.batch_size * len(dataloader)  # ※全データ =  (バッチ内のデータ数) * (バッチ数)\n",
        "                                  # 予測結果はバッチ毎に、バッチ内のデータ数だけ求まる\n",
        "    correct, loss = 0, 0\n",
        "\n",
        "    with torch.no_grad():  # テスト中は伝播記録をしない\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)           \n",
        "            pred = model(X)\n",
        "            loss += loss_fn(pred, y).item()  # バッチ内に複数データがあるなら、各データの損失の平均を返す?\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    return loss / total_loss, correct / total_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVT2i_zLlp1v"
      },
      "source": [
        "import time\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, loss_fn, optimizer, epochs, device=None):\n",
        "\n",
        "  # GPUに送る\n",
        "  model.to(device)\n",
        "\n",
        "  # dataloaderの作成\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "  # 学習\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(epochs):\n",
        "    # 開始時刻の記録\n",
        "    s_time = time.time()\n",
        "\n",
        "    # 訓練モード\n",
        "    # model.train()\n",
        "    for X, y in dataloader_train:\n",
        "      # 勾配をゼロで初期化\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # 損失と正解率の算出\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataloader_train, loss_fn, device)  # dataとlossの位置が参考と逆なので注意\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataloader_valid, loss_fn, device)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    # チェックポイントの保存\n",
        "    # torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    # 終了時刻の記録\n",
        "    e_time = time.time()\n",
        "\n",
        "    # ログを出力\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC9CxPjCmV2f"
      },
      "source": [
        "# datasetの作成\n",
        "dataset_train = NewsDataset(X_train, y_train)\n",
        "dataset_valid = NewsDataset(X_valid, y_valid)\n",
        "\n",
        "# モデルの定義\n",
        "model = NeuralNetwork(300, 4)\n",
        "\n",
        "# 損失関数の定義\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "# デバイスの指定\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# モデルの学習\n",
        "for batch_size in [2 ** i for i in range(11)]:\n",
        "  print(f'バッチサイズ: {batch_size}')\n",
        "  log = train_model(dataset_train, dataset_valid, batch_size, model, loss_fn, optimizer, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pf9znncKUyM"
      },
      "source": [
        "## 79. 多層ニューラルネットワーク\n",
        "\n",
        "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odG12XmMpRdD"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class DeepNN(nn.Module):  #  define our neural network by subclassing nn.Module\n",
        "  def __init__(self, input_size, output_size):  # initialize the neural network layers in __init__\n",
        "    super().__init__()\n",
        "    self.liner_stack = nn.Sequential(\n",
        "            nn.Linear(input_size,200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, output_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "  def forward(self, x):  # Every nn.Module subclass implements the forward method\n",
        "    x = self.liner_stack(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJGNPe7LocOB"
      },
      "source": [
        "def calculate_loss_and_accuracy(model, dataloader, loss_fn, device):\n",
        "    total_loss = len(dataloader)  # バッチ数(損失はバッチ毎(1イテレータ)に1つ求まるので)\n",
        "    total_data = dataloader.batch_size * len(dataloader)  # ※全データ =  (バッチ内のデータ数) * (バッチ数)\n",
        "                                  # 予測結果はバッチ毎に、バッチ内のデータ数だけ求まる\n",
        "    correct, loss = 0, 0\n",
        "\n",
        "    with torch.no_grad():  # テスト中は伝播記録をしない\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)           \n",
        "            pred = model(X)\n",
        "            loss += loss_fn(pred, y).item()  # バッチ内に複数データがあるなら、各データの損失の平均を返す?\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    return loss / total_loss, correct / total_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WDNQODzocOD"
      },
      "source": [
        "import time\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, loss_fn, optimizer, epochs, device=None):\n",
        "\n",
        "  # GPUに送る\n",
        "  model.to(device)\n",
        "\n",
        "  # dataloaderの作成\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "  # 学習\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(epochs):\n",
        "    # 開始時刻の記録\n",
        "    s_time = time.time()\n",
        "\n",
        "    # 訓練モード\n",
        "    # model.train()\n",
        "    for X, y in dataloader_train:\n",
        "      # 勾配をゼロで初期化\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # 損失と正解率の算出\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataloader_train, loss_fn, device)  # dataとlossの位置が参考と逆なので注意\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataloader_valid, loss_fn, device)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    # チェックポイントの保存\n",
        "    # torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    # 終了時刻の記録\n",
        "    e_time = time.time()\n",
        "\n",
        "    # ログを出力\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "    \n",
        "    # 検証データの損失が3エポック連続で低下しなかった場合は学習終了\n",
        "    if epoch > 2 and log_valid[epoch - 3][0] <= log_valid[epoch - 2][0] <= log_valid[epoch - 1][0] <= log_valid[epoch][0]:\n",
        "      break\n",
        "    \n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVMxJ_wxocOE"
      },
      "source": [
        "# datasetの作成\n",
        "dataset_train = NewsDataset(X_train, y_train)\n",
        "dataset_valid = NewsDataset(X_valid, y_valid)\n",
        "\n",
        "# モデルの定義\n",
        "model = DeepNN(300, 4)\n",
        "\n",
        "# 損失関数の定義\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "# デバイスの指定\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# モデルの学習\n",
        "batch_size = 64\n",
        "log = train_model(dataset_train, dataset_valid, batch_size, model, loss_fn, optimizer, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiDvaVBboV8m"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax[0].plot(np.array(log['train']).T[0], label='train')\n",
        "ax[0].plot(np.array(log['valid']).T[0], label='valid')\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].legend()\n",
        "ax[1].plot(np.array(log['train']).T[1], label='train')\n",
        "ax[1].plot(np.array(log['valid']).T[1], label='valid')\n",
        "ax[1].set_xlabel('epoch')\n",
        "ax[1].set_ylabel('accuracy')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}